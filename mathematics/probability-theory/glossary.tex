\longnewglossaryentry{event}{name=event}
{%
  Based on a \gls{event space} of possible outcomes \glssymbol{event space} and a set of measurable events \glssymbol{measurable set} to which we want to assign probabilities, an event $a \in \glssymbol{measurable set}$ is a subset of \glssymbol{event space}.\\
}

\longnewglossaryentry{event space}{name=event space,symbol={\ensuremath{\Omega}}}
{%
  A set of possible outcomes $\Omega$ with the following properties:
  \begin{itemize}
    \item It contains the empty event $\emptyset$, and the trivial event $\Omega$.
    \item It is closed under union: If $\alpha, \beta \in \glssymbol{measurable set}$, then so is $\alpha \cup \beta$.
    \item It is closed under complementation: If $\alpha \in \glssymbol{measurable set}$, then so is $\Omega - \alpha$.
  \end{itemize}
}

\longnewglossaryentry{measurable set}{name=measurable set,symbol=\ensuremath{\mathcal{S}}}
{%
  A set $\mathcal{S}$ is measurable if it possible to assign a number to each suitable subset of $\mathcal{S}$.
}

\longnewglossaryentry{probability distribution}{name=probability distribution}
{%
  A probability distribution $P$ over $(\glssymbol{event space},\glssymbol{measurable set})$ is a mapping from events in \glssymbol{measurable set} to real values that satisfies the following conditions:
  \begin{itemize}
    \item $P(\alpha)\geq 0 $ for all $ \alpha \in S$.
    \item $P(\glssymbol{event space})=1$.
    \item If $\alpha,\beta\in \glssymbol{event space}$ and $\alpha\cap\beta = \emptyset$, then $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$.
  \end{itemize}
  Interesting conditions that are implied from these:
  \begin{itemize}
    \item $P(\emptyset)=0$
    \item $P(\alpha\cup\beta)=P(\alpha)+P(\beta)-P(\alpha\cap\beta)$
  \end{itemize}
}

\longnewglossaryentry{conditional probability}{name=conditional probability}
{%
  The conditional probability of $\beta$ given $\alpha$ with $P(\alpha)>0$ is defined as:
  \begin{equation*}
    P(\beta\mid\alpha)=\frac{P(\alpha\cap\beta)}{P(\alpha)}
  \end{equation*}
  $P(\beta\mid\alpha)$ is a \gls{probability distribution} since it satisfies its properties.
}

\longnewglossaryentry{conditional distribution}{name=conditional distribution}
{%
  A \gls{probability distribution} in the form of $P(X\mid Y)$ where for each value of $Y$ this object assigns a probability over values of $X$ using the \gls{conditional probability}. It is only defined for non-zero $P(Y=y)$. $Y$ represents \textit{prior} knowledge.
}


\longnewglossaryentry{chain rule}{name=chain rule}
{%
  Based on the definition of \gls{conditional probability}, for events $\alpha$ and $\beta$:
  \begin{equation*}
    P(\alpha\cap\beta)=P(\alpha)P(\beta\mid\alpha)
  \end{equation*}
  In general:
  \begin{equation*}
    _P(\alpha_1\cap\cdots\cap\alpha_k)=P(\alpha_1)P(\alpha_2\mid\alpha_1)\cdots P(\alpha_k\mid\alpha_1\cap\cdots\cap\alpha_{k-1})
  \end{equation*}\\[0.1cm]
  For \glspl{random variable} $X$,$Y$:
  \begin{equation*}
    P(X,Y)=P(X)P(Y\mid X)
  \end{equation*}
  In general:
  \begin{equation*}
    P(X_1,\dots X_k)=P(X_1)P(X_2\mid X_1)\cdots P(X_k\mid X_1,\cdots,X_{k-1})
  \end{equation*}

}

\longnewglossaryentry{bayes rule}{name=Bayes' rule}
{%
  Derived from the definition of \gls{conditional probability}:
  \begin{equation*}
    P(\alpha\mid\beta)=\frac{P(\alpha)P(\beta\mid\alpha)}{P(\beta)}
  \end{equation*}\\[0.1cm]

  For \glspl{random variable} $X$,$Y$:
  \begin{equation*}
    P(X\mid Y)=\frac{P(X)P(Y\mid X)}{P(Y)}
  \end{equation*}
}

\longnewglossaryentry{random variable}{name=random variable}
{%
  A function that associates with each outcome in \glssymbol{event space} a value.

  Example: $f_{Grade}$ associates with every person in \glssymbol{event space} a grade (A,B,C). The event $Grade=A$ is short for the event $\{w\in\Omega:f_{Grade}(w)=A\}$. We can then use expressions like $P(Grade=A)$.\\

  Notation:
  \begin{itemize}
    \item $X$,$Y$,$Z$... for random variables
    \item $x$,$y$,$z$... for assigments of values to  $X$,$Y$,$Z$: $P(X=x)\geq0 $ for all $x\in \gls{val}(X)$
    \item $P(x)$ is short for $P(X=x)$
    \item $\sum_x$ is a sum over all possble values that $X$ can take: $\sum_xP(x)=1$
  \end{itemize}
}

\longnewglossaryentry{set of random variables}{name=set of random variables, plural=sets of random variables}
{%
  Notation:
  \begin{itemize}
    \item $\bm{X}$,$\bm{Y}$,$\bm{Z}$... for sets of \glspl{random variable}
    \item $\bm{x}$,$\bm{y}$,$\bm{z}$... for assigments of values to the random varibales in the sets $X$,$Y$,$Z$
    \item For $\bm{Y}\subseteq \bm{X}$, $\bm{x}\langle \bm{Y}\rangle$ is the assignment within $\bm{x}$ to the variables in $\bm{Y}$.
    \item For two assignments $\bm{x}$ and $\bm{y}$, $\bm{x}\sim \bm{y}$ if they agree on the variables in their intersection: $\bm{x}\langle\bm{X}\cap\bm{Y}\rangle=\bm{y}\langle\bm{X}\cap\bm{Y}\rangle$
  \end{itemize}
}

\longnewglossaryentry{categorical random variable}{name=categorical random variable}
{%
  A \gls{random variable} that can be expressed with separate probabilities specified for each of the $k$ possible outcomes.

  Notation:
  \begin{itemize}
    \item $x^1$,$x^2$,...$x^k$ for $k=\lvert\gls{val}(X)\rvert$ to enumerate the specific values of $X$.
  \end{itemize}
}

\longnewglossaryentry{val}{name=\ensuremath{Val},sort=Val}
{%
  For a \gls{random variable} $X$, $Val(X)$ is the set of values that $X$ can take.

  For a \gls{set of random variables} $\bm{X}$, $Val(\bm{X})$ is the set of possible assignments $\bm{x}$ to $\bm{X}$.
}

\longnewglossaryentry{multinomial distribution}{name=multinomial distribution}
{%
  A distribution over $x^1,\cdots,x^k$, for $\lvert\gls{val}(X)\rvert$ with $\sum_{i=1}^k P(x^i)=1$.
}

\longnewglossaryentry{bernoulli distribution}{name=Bernoulli distribution}
{%
  A \gls{multinomial distribution} with $\gls{val}(X)=\{false,true\}$. Typically, $x^0$ denotes the value \textit{false} and $x^1$ the value \textit{true}.
}

\longnewglossaryentry{marginal distribution}{name=marginal distribution}
{%
  A \gls{probability distribution} over events that can be described using $X$, denoted by $P(X)$. The only difference to the definition of a \gls{probability distribution} is that here we are restricted to the subsets of \glssymbol{measurable set} that can be described with $X$.
}

\longnewglossaryentry{joint distribution}{name=joint distribution}
{%
  A \gls{probability distribution} over a set $\bm{X}=\{X_1,\cdots,X_n\}$, denoted with $P(X_1,\cdots,X_n)$. It assigns probabilities to events that are specified in terms of the \glspl{random variable}. $\xi$ refers to a full assignment to the variables in $\bm{X}$: $\xi\in\glslink{val}{Val(\mathcal{X})}$

  Properties:
  \begin{itemize}
    \item The joint distribution of 2 variables has to be consistent with the \gls{marginal distribution}: $P(x)=\sum_{y}P(x,y)$
    \item The most fine-grained event possible is $X_1=x_1$ and $X_1=x_1$,$\cdots$, and $X_n=x_n$ for a choice of values $x_1,\cdots,x_n$.
    \begin{itemize}
      \item Two of this events are either identical or disjoint.
      \item Any event defined using variables in $\bm{X}$ must be a union of a set of such events.
    \end{itemize}
    \item The probability computations remain the same whether we consider the original outcome space or the \gls{canonical outcome space}.
  \end{itemize}

}

\longnewglossaryentry{canonical outcome space}{name=canonical outcome space}
{%
  An \gls{event space} where each outcome corresponds to a \glslink{joint distribution}{joint assignment} to $X_1,\dots, X_n$.
}

\longnewglossaryentry{atomic outcome}{name=atomic outcome, symbol=\ensuremath{\xi}}
{%
  An outcome that assigns a value to each \gls{random variable} in a set of random variables $\bm{X}$.
}

\longnewglossaryentry{independence}{name=independence}
{%
  An event $\alpha$ is independent of event $\beta$ in $P$, denoted $P\models(\alpha\perp\beta)$, if $P(\alpha\mid\beta)=P(\alpha)$ or if $P(\beta)=0$.

  Alternatively: A \gls{probability distribution} $P$ satisfies $(\alpha\perp\beta\mid\gamma)$ \acrshort{iff} $P(\alpha\cap\beta\mid\gamma)=P(\alpha\mid\gamma)P(\beta\mid\gamma)$.
}

\longnewglossaryentry{conditional independence}{name=conditional independence}
{%
  Givent events $\alpha$, $\beta$, and $\gamma$: $\alpha$ is conditionally independent of event $\beta$ given event $\gamma$ in $P$, denoted $P\models(\alpha\perp\beta\mid\gamma)$, if $P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ or if $P(\beta\cap\gamma)=0$.

  Alternatively: A \gls{probability distribution} $P$ satisfies $(\alpha\perp\beta)$ \acrshort{iff} $P(\alpha\cap\beta)=P(\alpha)P(\beta)$.\\[0.1cm]

  Given sets of random variables $\bm{X}$, $\bm{Y}$, and $\bm{Z}$: $\bm{X}$ is conditionally independent of $\bm{Y}$ given $\bm{Z}$ in $P$, denoted $P\models(\bm{X}\perp\bm{Y}\mid\bm{Z})$, if $P(\bm{X}\mid\bm{Y}\cap\bm{Z})=P(\bm{X}\mid\bm{Z})$ or if $P(\bm{Y}\cap\bm{Z})=0$.

  Alternatively: A \gls{probability distribution} $P$ satisfies $(\bm{X}\perp\bm{Y})$ \acrshort{iff} $P(\bm{X}\cap\bm{Y})=P(\bm{X})P(\bm{Y})$.

  For conditional independence, the following independence properties also hold: \gls{symmetry}, \gls{decomposition}, \gls{weak union}, \gls{contraction}
}

\longnewglossaryentry{symmetry}{name=symmetry}
{%
  Given sets of random variables $\bm{X}$, $\bm{Y}$, and $\bm{Z}$:
  \begin{equation*}
    (\bm{X}\perp\bm{Y}\mid\bm{Z})\implies(\bm{Y}\perp\bm{X}\mid\bm{Z})
  \end{equation*}
}

\longnewglossaryentry{decomposition}{name=decomposition}
{%
  Given sets of random variables $\bm{W}$, $\bm{X}$, $\bm{Y}$, and $\bm{Z}$:
  \begin{equation*}
    (\bm{X}\perp\bm{Y},\bm{W}\mid\bm{Z})\implies(\bm{X}\perp\bm{Y}\mid\bm{Z})
  \end{equation*}
}

\longnewglossaryentry{weak union}{name=weak union}
{%
  Given sets of random variables $\bm{W}$, $\bm{X}$, $\bm{Y}$, and $\bm{Z}$:
  \begin{equation*}
    (\bm{X}\perp\bm{Y},\bm{W}\mid \bm{Z})\implies(\bm{X}\perp\bm{Y}\mid\bm{Z},\bm{W})
  \end{equation*}
}

\longnewglossaryentry{contraction}{name=contraction}
{%
  Given sets of random variables $\bm{W}$, $\bm{X}$, $\bm{Y}$, and $\bm{Z}$:
  \begin{equation*}
    (\bm{X}\perp\bm{W}\mid\bm{Z},\bm{Y})~\&~(\bm{X}\perp\bm{Y}\mid\bm{Z})\implies(\bm{X}\perp\bm{Y},\bm{W}\mid\bm{Z})
  \end{equation*}
}

\longnewglossaryentry{positive distribution}{name=positive distribution}
{%
  A \gls{probability distribution} $P$ where for all events $\alpha\in\glssymbol{measurable set}$ such that $\alpha\neq\emptyset$, we have that $P(\alpha)>0$.

  Positive distributions have the property of \gls{intersection}.
}

\longnewglossaryentry{intersection}{name=intersection}
{%
  For \glspl{positive distribution}, and for mutually disjoint \glspl{set of random variables} $\bm{W}$, $\bm{X}$, $\bm{Y}$, and $\bm{Z}$:
  \begin{equation*}
    (\bm{X}\perp\bm{Y}\mid\bm{Z},\bm{W})~\&~(\bm{X}\perp\bm{W}\mid\bm{Z},\bm{Y})\implies(\bm{X}\perp\bm{Y},\bm{W}\mid\bm{Z})
  \end{equation*}
}

\longnewglossaryentry{probability query}{name=probability query}
{%
  %TODO define "query"
  A query that consists of the \gls{evidence} $\bm{E}$ and \gls{query variables} $\bm{Y}$.

  The task is to compute the \gls{posterior probability distribution} $P(\bm{Y}\mid\bm{E}=\bm{e})$.
}

\longnewglossaryentry{evidence}{name=evidence}
{%
  A subset $\bm{E}$ of \glspl{random variable} , and an instantiation $\bm{e}$ to these variables. Part of a \gls{probability query}.
}

\longnewglossaryentry{query variables}{name=query variables}
{%
  A subset $\bm{Y}$ of \glspl{random variable} in a given network. Part of a \gls{probability query}.
}

\longnewglossaryentry{posterior probability distribution}{name=posterior probability distribution}
{%
  A \gls{probability distribution} over the values $\bm{y}$ of $\bm{Y}$, conditioned on the fact that $\bm{E}=\bm{e}$:
  \begin{equation*}
    P(\bm{Y}\mid\bm{E}=\bm{e})
  \end{equation*}

  Alternative definition: A marginal over $\bm{Y}$, in the distibution obtained by \gls{conditioning} on $\bm{e}$.
}

\longnewglossaryentry{conditioning}{name=conditioning}
{%
  %TODO find better description
  The concept that beliefs depend on available information. See \gls{conditional probability}.
}

